<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>BIP.Bayes.Samplers &mdash; BIP - Bayesian Inference with Python 0.5.13 documentation</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.5.13',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="BIP - Bayesian Inference with Python 0.5.13 documentation" href="index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="index.html">BIP</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/BIP.png" alt="Logo"/>
            </a></p>
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">BIP.Bayes.Samplers</a><ul>
<li><a class="reference internal" href="#module-BIP.Bayes.Samplers.MCMC">MCMC</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/BIP.Bayes.Samplers.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="module-BIP.Bayes.Samplers">
<span id="bip-bayes-samplers"></span><h1>BIP.Bayes.Samplers<a class="headerlink" href="#module-BIP.Bayes.Samplers" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-BIP.Bayes.Samplers.MCMC">
<span id="mcmc"></span><h2>MCMC<a class="headerlink" href="#module-BIP.Bayes.Samplers.MCMC" title="Permalink to this headline">¶</a></h2>
<p>Module implementing MCMC samplers</p>
<blockquote>
<div><ul class="simple">
<li>Metropolis: Adaptive Metropolis Hastings sampler</li>
<li>Dream: DiffeRential Evolution Adaptive Markov chain sampler</li>
</ul>
</div></blockquote>
<dl class="class">
<dt id="BIP.Bayes.Samplers.MCMC.Dream">
<em class="property">class </em><tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">Dream</tt><big>(</big><em>meldobj</em>, <em>samples</em>, <em>sampmax</em>, <em>data</em>, <em>t</em>, <em>parpriors</em>, <em>parnames</em>, <em>parlimits</em>, <em>likfun</em>, <em>likvariance</em>, <em>burnin</em>, <em>thin=5</em>, <em>convergenceCriteria=1.1</em>, <em>nCR=3</em>, <em>DEpairs=1</em>, <em>adaptationRate=0.65</em>, <em>eps=5e-06</em>, <em>mConvergence=False</em>, <em>mAccept=False</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.Dream" title="Permalink to this definition">¶</a></dt>
<dd><p>DiffeRential Evolution Adaptive Markov chain sampler</p>
<dl class="method">
<dt id="BIP.Bayes.Samplers.MCMC.Dream.delayed_rejection">
<tt class="descname">delayed_rejection</tt><big>(</big><em>xi</em>, <em>zi</em>, <em>pxi</em>, <em>zprob</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.Dream.delayed_rejection" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates a second proposal based on rejected proposal xi</p>
</dd></dl>

<dl class="method">
<dt id="BIP.Bayes.Samplers.MCMC.Dream.step">
<tt class="descname">step</tt><big>(</big><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.Dream.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Does the actual sampling loop.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="BIP.Bayes.Samplers.MCMC.Metropolis">
<em class="property">class </em><tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">Metropolis</tt><big>(</big><em>meldobj</em>, <em>samples</em>, <em>sampmax</em>, <em>data</em>, <em>t</em>, <em>parpriors</em>, <em>parnames</em>, <em>parlimits</em>, <em>likfun</em>, <em>likvariance</em>, <em>burnin</em>, <em>**kwargs</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.Metropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>Standard random-walk Metropolis Hastings sampler class</p>
<dl class="method">
<dt id="BIP.Bayes.Samplers.MCMC.Metropolis.step">
<tt class="descname">step</tt><big>(</big><em>nchains=1</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.Metropolis.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Does the actual sampling loop.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="BIP.Bayes.Samplers.MCMC.model_as_ra">
<tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">model_as_ra</tt><big>(</big><em>theta</em>, <em>model</em>, <em>phinames</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.model_as_ra" title="Permalink to this definition">¶</a></dt>
<dd><p>Does a single run of self.model and returns the results as a record array</p>
</dd></dl>

<dl class="function">
<dt id="BIP.Bayes.Samplers.MCMC.multinomial">
<tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">multinomial</tt><big>(</big><em>n</em>, <em>pvals</em>, <em>size=None</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.multinomial" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from a multinomial distribution.</p>
<p>The multinomial distribution is a multivariate generalisation of the
binomial distribution.  Take an experiment with one of <tt class="docutils literal"><span class="pre">p</span></tt>
possible outcomes.  An example of such an experiment is throwing a dice,
where the outcome can be 1 through 6.  Each sample drawn from the
distribution represents <cite>n</cite> such experiments.  Its values,
<tt class="docutils literal"><span class="pre">X_i</span> <span class="pre">=</span> <span class="pre">[X_0,</span> <span class="pre">X_1,</span> <span class="pre">...,</span> <span class="pre">X_p]</span></tt>, represent the number of times the outcome
was <tt class="docutils literal"><span class="pre">i</span></tt>.</p>
<dl class="docutils">
<dt>n <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of experiments.</dd>
<dt>pvals <span class="classifier-delimiter">:</span> <span class="classifier">sequence of floats, length p</span></dt>
<dd>Probabilities of each of the <tt class="docutils literal"><span class="pre">p</span></tt> different outcomes.  These
should sum to 1 (however, the last element is always assumed to
account for the remaining probability, as long as
<tt class="docutils literal"><span class="pre">sum(pvals[:-1])</span> <span class="pre">&lt;=</span> <span class="pre">1)</span></tt>.</dd>
<dt>size <span class="classifier-delimiter">:</span> <span class="classifier">tuple of ints</span></dt>
<dd>Given a <cite>size</cite> of <tt class="docutils literal"><span class="pre">(M,</span> <span class="pre">N,</span> <span class="pre">K)</span></tt>, then <tt class="docutils literal"><span class="pre">M*N*K</span></tt> samples are drawn,
and the output shape becomes <tt class="docutils literal"><span class="pre">(M,</span> <span class="pre">N,</span> <span class="pre">K,</span> <span class="pre">p)</span></tt>, since each sample
has shape <tt class="docutils literal"><span class="pre">(p,)</span></tt>.</dd>
</dl>
<p>Throw a dice 20 times:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mf">6.</span><span class="p">]</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([[4, 1, 7, 5, 2, 1]])</span>
</pre></div>
</div>
<p>It landed 4 times on 1, once on 2, etc.</p>
<p>Now, throw the dice 20 times, and 20 times again:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mf">6.</span><span class="p">]</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">array([[3, 4, 3, 3, 4, 3],</span>
<span class="go">       [2, 4, 3, 4, 0, 7]])</span>
</pre></div>
</div>
<p>For the first run, we threw 3 times 1, 4 times 2, etc.  For the second,
we threw 2 times 1, 4 times 2, etc.</p>
<p>A loaded dice is more likely to land on number 6:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="mf">7.</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">)</span>
<span class="go">array([13, 16, 13, 16, 42])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="BIP.Bayes.Samplers.MCMC.multivariate_normal">
<tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">multivariate_normal</tt><big>(</big><em>mean</em>, <em>cov</em><span class="optional">[</span>, <em>size</em><span class="optional">]</span><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.multivariate_normal" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw random samples from a multivariate normal distribution.</p>
<p>The multivariate normal, multinormal or Gaussian distribution is a
generalization of the one-dimensional normal distribution to higher
dimensions.  Such a distribution is specified by its mean and
covariance matrix.  These parameters are analogous to the mean
(average or &#8220;center&#8221;) and variance (standard deviation, or &#8220;width,&#8221;
squared) of the one-dimensional normal distribution.</p>
<dl class="docutils">
<dt>mean <span class="classifier-delimiter">:</span> <span class="classifier">1-D array_like, of length N</span></dt>
<dd>Mean of the N-dimensional distribution.</dd>
<dt>cov <span class="classifier-delimiter">:</span> <span class="classifier">2-D array_like, of shape (N, N)</span></dt>
<dd>Covariance matrix of the distribution.  Must be symmetric and
positive semi-definite for &#8220;physically meaningful&#8221; results.</dd>
<dt>size <span class="classifier-delimiter">:</span> <span class="classifier">int or tuple of ints, optional</span></dt>
<dd>Given a shape of, for example, <tt class="docutils literal"><span class="pre">(m,n,k)</span></tt>, <tt class="docutils literal"><span class="pre">m*n*k</span></tt> samples are
generated, and packed in an <cite>m</cite>-by-<cite>n</cite>-by-<cite>k</cite> arrangement.  Because
each sample is <cite>N</cite>-dimensional, the output shape is <tt class="docutils literal"><span class="pre">(m,n,k,N)</span></tt>.
If no shape is specified, a single (<cite>N</cite>-D) sample is returned.</dd>
</dl>
<dl class="docutils">
<dt>out <span class="classifier-delimiter">:</span> <span class="classifier">ndarray</span></dt>
<dd><p class="first">The drawn samples, of shape <em>size</em>, if that was provided.  If not,
the shape is <tt class="docutils literal"><span class="pre">(N,)</span></tt>.</p>
<p class="last">In other words, each entry <tt class="docutils literal"><span class="pre">out[i,j,...,:]</span></tt> is an N-dimensional
value drawn from the distribution.</p>
</dd>
</dl>
<p>The mean is a coordinate in N-dimensional space, which represents the
location where samples are most likely to be generated.  This is
analogous to the peak of the bell curve for the one-dimensional or
univariate normal distribution.</p>
<p>Covariance indicates the level to which two variables vary together.
From the multivariate normal distribution, we draw N-dimensional
samples, <img class="math" src="_images/math/75799fb61cb7e0225f1a28c263b0199be87a2c9c.png" alt="X = [x_1, x_2, ... x_N]"/>.  The covariance matrix
element <img class="math" src="_images/math/949e305594f67ec0c0af08516116e44b0055a318.png" alt="C_{ij}"/> is the covariance of <img class="math" src="_images/math/33dfc32d00ebd5c5791c824010a155d9e5630b6f.png" alt="x_i"/> and <img class="math" src="_images/math/31569129806385759f74600c60ef3a4dacfe0f3a.png" alt="x_j"/>.
The element <img class="math" src="_images/math/0d98acfe52a23e8bda0d9233c4b7440723c6e77e.png" alt="C_{ii}"/> is the variance of <img class="math" src="_images/math/33dfc32d00ebd5c5791c824010a155d9e5630b6f.png" alt="x_i"/> (i.e. its
&#8220;spread&#8221;).</p>
<p>Instead of specifying the full covariance matrix, popular
approximations include:</p>
<blockquote>
<div><ul class="simple">
<li>Spherical covariance (<em>cov</em> is a multiple of the identity matrix)</li>
<li>Diagonal covariance (<em>cov</em> has non-negative elements, and only on
the diagonal)</li>
</ul>
</div></blockquote>
<p>This geometrical property can be seen in two dimensions by plotting
generated data-points:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">]]</span> <span class="c"># diagonal covariance, points lie on x or y-axis</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">cov</span><span class="p">,</span><span class="mi">5000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s">&#39;x&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">&#39;equal&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Note that the covariance matrix must be non-negative definite.</p>
<p>Papoulis, A., <em>Probability, Random Variables, and Stochastic Processes</em>,
3rd ed., New York: McGraw-Hill, 1991.</p>
<p>Duda, R. O., Hart, P. E., and Stork, D. G., <em>Pattern Classification</em>,
2nd ed., New York: Wiley, 2001.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span><span class="n">cov</span><span class="p">,(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(3, 3, 2)</span>
</pre></div>
</div>
<p>The following is probably true, given that 0.6 is roughly twice the
standard deviation:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="nb">list</span><span class="p">(</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.6</span> <span class="p">)</span>
<span class="go">[True, True]</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="BIP.Bayes.Samplers.MCMC.rand">
<tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">rand</tt><big>(</big><em>d0</em>, <em>d1</em>, <em>...</em>, <em>dn</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.rand" title="Permalink to this definition">¶</a></dt>
<dd><p>Random values in a given shape.</p>
<p>Create an array of the given shape and propagate it with
random samples from a uniform distribution
over <tt class="docutils literal"><span class="pre">[0,</span> <span class="pre">1)</span></tt>.</p>
<dl class="docutils">
<dt>d0, d1, ..., dn <span class="classifier-delimiter">:</span> <span class="classifier">int, optional</span></dt>
<dd>The dimensions of the returned array, should all be positive.
If no argument is given a single Python float is returned.</dd>
</dl>
<dl class="docutils">
<dt>out <span class="classifier-delimiter">:</span> <span class="classifier">ndarray, shape <tt class="docutils literal"><span class="pre">(d0,</span> <span class="pre">d1,</span> <span class="pre">...,</span> <span class="pre">dn)</span></tt></span></dt>
<dd>Random values.</dd>
</dl>
<p>random</p>
<p>This is a convenience function. If you want an interface that
takes a shape-tuple as the first argument, refer to
np.random.random_sample .</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="go">array([[ 0.14022471,  0.96360618],  #random</span>
<span class="go">       [ 0.37601032,  0.25528411],  #random</span>
<span class="go">       [ 0.49313049,  0.94909878]]) #random</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="BIP.Bayes.Samplers.MCMC.random">
<tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">random</tt><big>(</big><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.random" title="Permalink to this definition">¶</a></dt>
<dd><p>random_sample(size=None)</p>
<p>Return random floats in the half-open interval [0.0, 1.0).</p>
<p>Results are from the &#8220;continuous uniform&#8221; distribution over the
stated interval.  To sample <img class="math" src="_images/math/8e9ceb34b646b7fb9f8177f0db29fa0c2c0d71a8.png" alt="Unif[a, b), b &gt; a"/> multiply
the output of <cite>random_sample</cite> by <cite>(b-a)</cite> and add <cite>a</cite>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">random_sample</span><span class="p">()</span> <span class="o">+</span> <span class="n">a</span>
</pre></div>
</div>
<dl class="docutils">
<dt>size <span class="classifier-delimiter">:</span> <span class="classifier">int or tuple of ints, optional</span></dt>
<dd>Defines the shape of the returned array of random floats. If None
(the default), returns a single float.</dd>
</dl>
<dl class="docutils">
<dt>out <span class="classifier-delimiter">:</span> <span class="classifier">float or ndarray of floats</span></dt>
<dd>Array of random floats of shape <cite>size</cite> (unless <tt class="docutils literal"><span class="pre">size=None</span></tt>, in which
case a single float is returned).</dd>
</dl>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">()</span>
<span class="go">0.47108547995356098</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">())</span>
<span class="go">&lt;type &#39;float&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">5</span><span class="p">,))</span>
<span class="go">array([ 0.30220482,  0.86820401,  0.1654503 ,  0.11659149,  0.54323428])</span>
</pre></div>
</div>
<p>Three-by-two array of random numbers from [-5, 0):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">-</span> <span class="mi">5</span>
<span class="go">array([[-3.99149989, -0.52338984],</span>
<span class="go">       [-2.99091858, -0.79479508],</span>
<span class="go">       [-1.23204345, -1.75224494]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="BIP.Bayes.Samplers.MCMC.timeit">
<tt class="descclassname">BIP.Bayes.Samplers.MCMC.</tt><tt class="descname">timeit</tt><big>(</big><em>method</em><big>)</big><a class="headerlink" href="#BIP.Bayes.Samplers.MCMC.timeit" title="Permalink to this definition">¶</a></dt>
<dd><p>Decorator to time methods</p>
</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="index.html">BIP</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010-14, Flávio Codeço Coelho.
      Last updated on May 27, 2014.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>