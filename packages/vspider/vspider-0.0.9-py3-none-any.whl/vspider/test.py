import vthread
import vspider

# vspider 是一个使用 sqlite 对轻量级文本进行数据收集的爬虫库
# 线程安全，配合 vthread 相当方便

f = lambda i:i[:20]

@vthread.pool(3) # 开三个线程池
def some(url):
    print(url)
    x @ url
    x << 'string(//*[@id="1"]/h3/a[1])'
    x << '//*[@id="2"]/h3/a/text()'
    # 最简化：
    # 以上方法将会在 sqlite 里面生成一个表名为 some（默认用函数名字），
    # 所有列名为 col_0, col_1 的表
    # 每次执行该函数就会用默认的函数对 url 解析获取其content
    # 然后以各列的 xpath 解析 content 获取录入数据

    x | "foo"
    x("foo") @ url
    x << ("col1",'//*[@id="1"]/h3/a[1]/@href')
    x << ("col2",'//*[@id="2"]/h3/a/@href')
    # 可配置：
    # 以上方法将会在 sqlite 里面生成一个表名为 foo，
    # 所有列名为 col1, col2 的表
    # 每次执行该函数就会用默认的函数对 url 解析获取其content
    # 然后以各列的 xpath 解析 content 获取录入数据
    # 且到这里，some 表和 foo 表以及表收集的数据互不干扰

    # 为了 content 的复用
    # 如果你想使用上一张表获取的 content 请直接用 x(table_name) & x 即可

    import requests
    content = requests.get(url).content
    
    x("asdf") & content
    x << ("col1",'//*[@id="1"]/h3/a[1]/@href',f)
    x << ("col0",'//*[@id="2"]/h3/a/@href')
    # 由于 vspider 自带的网页 html_content 获取的功能不够强大
    # 有时你需要通过别的库获取 html_content 然后通过 & 传入即可
    # @ 和 & 在同名表中请不要重复使用
    # 当你使用list或tuple来传递结构的时候，可以通过第三个参数来配置处理数据的函数
    # 比如上面的配置会让收集到的col1的数据会使用第三个参数的方法进行数据处理
    # 处理完之后才会输入数据库，默认参数是一个lambda i:i.strip()
    # 如果你有需要的数据，前后的空格都需要放进去，那么就写入None即可
    # 如：
    # x << ("col1",'//*[@id="1"]/h3/a[1]/@href',None)

    # 注意：
    # 两个配置表名字的中间的所有 col 配置都为前一个表的 col 配置
    # 使用list或tuple配置收集结构的时候，列名和xpath是必填的


url = 'http://www.baidu.com/s?wd=翻译'
for i in range(5):
    some(url);some(url)

@vthread.pool(3)
def some2(url):
    x @ url
    x * '//*[@class="result c-container "]'
    x ** 'string(./h3/a)'
    x ** 'string(./h3/a/@href)'
    x * '//*[@class="result-op c-container"]'
    x ** 'string(./h3/a)'
    x ** 'string(./h3/a/@href)'

url = 'http://www.baidu.com/s?wd=翻译'
for i in range(5):
    some2(url)



def some3(url):
    print(url)
    x("真臭") @ url
    x * '//*[contains(@class,"c-container")]'
    x ** ("标题",'string(./h3/a)')
    x ** ("链接",'string(./h3/a/@href)',lambda i:i[26:]) # 测试爬去数据的后续处理
    x ** ("简介",'string(./div)')

    if url.endswith("=0"):
        x("真香的菜") @ url
        x << 'string(//*[@id="1"]/h3/a)'
        x << 'string(//*[@id="2"]/h3/a)'

for i in range(5):
    url = f"https://www.baidu.com/s?wd=你好&pn={i*10}"
    some3(url)





import vspider,vthread

@vthread.pool(10)
def some4(url):
    print(url)
    x("真可怕") @ url
    x * '//*[contains(@class,"c-container")]'
    x ** ("标题",'string(./h3/a)')
    x ** ("链接",'string(./h3/a/@href)',lambda i:i[26:]) # 测试xpath获取数据的后续处理
    x ** ("简介",'string(./div)')
    x + ('//*[@id="page"]/a/@href',lambda i:'&'.join(i.split('&')[:2]))

# 自动翻页设计，将其设计为迭代器
# 重载函数 + ，如果能从content里面解析出下页就将其加入迭代器，迭代器就自动将其迭代出来
u = "https://www.baidu.com/s?wd=你好&pn=0"
for i in x.start_url(u):
    some4(i)









