# Copyright 2017 StreamSets Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# This YAML file is used by STE to determine each environment's start and stop execution.

# Following are various inbuilt defaults which can be used by environments:
# {docker-network} - The docker network used
# {testenvironments-config-directory} - Testenvironments config directory
# {environment} - The environment name which is being used
# {random-name} - A random name
# {extra-arguments} - Extra arguments which need to be passed from command line to a command are referred by this

# Notes:
# 1. A dictionary key suffixed with + or multiples of it signifies merging with other key(s) with same name.
# The order of merge is based on ascending number of pluses (+) in the suffix.
# E.g., The 'start' keys will be merged in the order as: start, start+, start++, start+++
# 2. YAML values starting with { needs to be quoted (single or double).
# 3. YAML values within which need single quotes needs to be enclosed in double quotes or vice-versa.
# 4. Any value which needs explicit start and end braces (e.g., {something}),
# needs them to be enclosed again in braces (e.g., {{something}})
# 5. Due to shebang issue (long dir paths - https://github.com/pypa/pip/issues/1773) we run Python commands
# indirectly using `python` interpreter instead.

defaults:
    # Uncomment the below to add args which can be exposed via argsparse and can be used as defaults for environments.
    # args:

    SUPER_COMMAND: docker run --rm -v {testenvironments-config-directory}:{testenvironments-config-directory} busybox

    CLUSTERDOCK_START_ARGS: '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python
        {testenvironments-config-directory}/{environment}/workspace/venv/bin/clusterdock -v
        --clusterdock-config-directory {testenvironments-config-directory}/{environment} start
        --network {docker-network}'

    CLUSTERDOCK_START_ARGS_SS: '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python
        {testenvironments-config-directory}/{environment}/workspace/venv/bin/clusterdock -v
        --clusterdock-config-directory {testenvironments-config-directory}/{environment} start
        --network {docker-network} --namespace streamsets'

    CLUSTERDOCK_CDH_ARGS: '{testenvironments-config-directory}/{environment}/workspace/topology_cdh --java jdk1.8.0_131'

    CLUSTERDOCK_HDP_ARGS: '{testenvironments-config-directory}/{environment}/workspace/topology_hdp'

    CLUSTERDOCK_KAFKA_ARGS: '{testenvironments-config-directory}/{environment}/workspace/topology_apache_kafka
        --brokers node-1 node-2 node-3'
    CLUSTERDOCK_SCHEMA_REGISTRY_ARGS:
        '{testenvironments-config-directory}/{environment}/workspace/topology_confluent_schema_registry
            --nodes registry-1'

    CLUSTERDOCK_MAPR_ARGS: "{testenvironments-config-directory}/{environment}/workspace/topology_mapr
        --node-disks '{{node-1:[/dev/xvdb],node-2:[/dev/xvdc]}}'"

    CLUSTERDOCK_DSE_ARGS: '{testenvironments-config-directory}/{environment}/workspace/topology_dse --kerberos
        --kerberos-config-directory {testenvironments-config-directory}/{environment} --kerberos-principals sdc'

    # Note the principal is not sdc since it conflicts
    # with CM generated principal sdc/node-1.cluster in case of SDC installed on kerberized CDH cluster.
    CLUSTERDOCK_KERBEROS_ARGS: --kerberos --kerberos-principals sdctest
    CLUSTERDOCK_KERBEROS_COPY:
        docker run --rm -v {testenvironments-config-directory}:{testenvironments-config-directory} busybox
            cp {testenvironments-config-directory}/{environment}/kerberos/*.*
                {testenvironments-config-directory}/{environment}
    CLUSTERDOCK_KERBEROS_KEYTAB_RENAME:
        docker run --rm -v {testenvironments-config-directory}:{testenvironments-config-directory} busybox
            mv -f {testenvironments-config-directory}/{environment}/clusterdock.keytab
                {testenvironments-config-directory}/{environment}/sdc.keytab

    CLUSTERDOCK_SSL_KEYS_CERTS_COPY:
        cp {testenvironments-config-directory}/{environment}/ssl/*pem {testenvironments-config-directory}/{environment}

    COUCHBASE_DEFAULT_USERNAME: admin
    COUCHBASE_DEFAULT_PASSWORD: admin123

    GCP_ENV_CONFIG_COMMANDS:
        gcloud auth activate-service-account --key-file /root/config/GCP-credentials.json &&
        gcloud config set project streamsets-engineering
    GCP_BIGTABLE_INSTANCE_CREATE_COMMAND:
        gcloud beta bigtable instances create $GCP_BIGTABLE_INSTANCE --cluster=$GCP_BIGTABLE_INSTANCE-cluster
            --cluster-zone=us-central1-c --display-name=$GCP_BIGTABLE_INSTANCE-disp --instance-type=DEVELOPMENT
    GCP_BIGTABLE_INSTANCE_DELETE_COMMAND:
        gcloud beta bigtable instances delete --quiet $GCP_BIGTABLE_INSTANCE

    AWS_CLI_PREFIX_COMMAND:
        docker run -e "AWS_ACCESS_KEY_ID=${{AWS_ACCESS_KEY_ID}}" -e "AWS_SECRET_ACCESS_KEY=${{AWS_SECRET_ACCESS_KEY}}"
        -v {testenvironments-config-directory}/{environment}:/project

    POSTGRESQL_CONFIG_FILES_COMMAND:
        -c 'config_file=/etc/postgresql/postgresql.conf'
        -c 'hba_file=/etc/postgresql/pg_hba.conf'

AWS_EMR_PRE: &AWS_EMR_PRE_DEFAULTS
    args:
        --aws-region:
            default: us-west-2
            help: AWS region to use
            metavar: region
        --aws-ec2-subnet-id:
            default: subnet-be9b31c9
            help: VPC subnet in which to create the cluster
            metavar: subnet-id
        --aws-ec2-security-group:
            default: sg-f9d2eb9c
            help: Security group ID of the Amazon EC2 security group for the master and slave nodes
            metavar: security-group
    start_help: Set env variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
        before starting this environment
    stop_help: Set env variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
        before stopping this environment

AWS_EMR_POST: &AWS_EMR_POST_DEFAULTS
    start+:
        - echo $(cat {testenvironments-config-directory}/{environment}/clusterid.txt |
            awk '/ClusterId/{{print $NF}}' | tr -d '"') >
            {testenvironments-config-directory}/{environment}/clusterid.txt
        - '{AWS_CLI_PREFIX_COMMAND} -e "AWS_DEFAULT_REGION={aws-region}" mesosphere/aws-cli $* emr wait cluster-running
            --cluster-id $(cat {testenvironments-config-directory}/{environment}/clusterid.txt)'
    stop:
        - '{AWS_CLI_PREFIX_COMMAND} -e "AWS_DEFAULT_REGION={aws-region}" mesosphere/aws-cli $* emr terminate-clusters
            --cluster-ids $(cat {testenvironments-config-directory}/{environment}/clusterid.txt)'

CDH_PRE: &CDH_PRE_DEFAULTS
    start:
        - python3 -m venv {testenvironments-config-directory}/{environment}/workspace/venv
        - git -C {testenvironments-config-directory}/{environment}/workspace clone
            https://github.com/clusterdock/topology_cdh.git
        - git -C {testenvironments-config-directory}/{environment}/workspace/topology_cdh checkout streamsets
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install clusterdock'
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install
            -r {testenvironments-config-directory}/{environment}/workspace/topology_cdh/requirements.txt'

CDH_POST: &CDH_POST_DEFAULTS
    start++:
        - echo "Waiting 1 minute for cluster services to stabilize ..."
        - sleep 60
    stf arguments:
        - --cluster-server cm://node-1.{docker-network}:7180
        - --sdc-server-url http://node-1.{docker-network}:18630

CDH_KERBEROS_POST: &CDH_KERBEROS_POST_DEFAULTS
    start++:
        - echo "Waiting 1 minute for cluster services to stabilize ..."
        - sleep 60
    stf arguments:
        - --cluster-server cm://node-1.{docker-network}:7180
        - --kerberos
        - --sdc-server-url http://node-1.{docker-network}:18630

DSE_PRE: &DSE_PRE_DEFAULTS
    start:
        - python3 -m venv {testenvironments-config-directory}/{environment}/workspace/venv
        - git -C {testenvironments-config-directory}/{environment}/workspace clone
            https://github.com/clusterdock/topology_dse.git
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install clusterdock'

HDP_PRE: &HDP_PRE_DEFAULTS
    start:
        - python3 -m venv {testenvironments-config-directory}/{environment}/workspace/venv
        - git -C {testenvironments-config-directory}/{environment}/workspace clone
            https://github.com/clusterdock/topology_hdp.git
        - git -C {testenvironments-config-directory}/{environment}/workspace/topology_hdp checkout streamsets
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install clusterdock'
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install
            -r {testenvironments-config-directory}/{environment}/workspace/topology_hdp/requirements.txt'

HDP_POST: &HDP_POST_DEFAULTS
    start++:
        - echo "Waiting 1 minute for cluster services to stabilize ..."
        - sleep 60
    stf arguments:
        - --cluster-server ambari://http://node-1.{docker-network}:8080

KAFKA_PRE: &KAFKA_PRE_DEFAULTS
    start:
        - python3 -m venv {testenvironments-config-directory}/{environment}/workspace/venv
        - git -C {testenvironments-config-directory}/{environment}/workspace clone
            https://github.com/clusterdock/topology_apache_kafka.git
        - git -C {testenvironments-config-directory}/{environment}/workspace clone
            https://github.com/clusterdock/topology_confluent_schema_registry
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install clusterdock'

MAPR_PRE: &MAPR_PRE_DEFAULTS
    start_help: Node disks (/dev/xvdb, /dev/xvdc) are assumed to be pre-created and available for use
    start:
        - python3 -m venv {testenvironments-config-directory}/{environment}/workspace/venv
        - git -C {testenvironments-config-directory}/{environment}/workspace clone
            https://github.com/clusterdock/topology_mapr.git
        - git -C {testenvironments-config-directory}/{environment}/workspace/topology_mapr checkout streamsets
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python -m pip install clusterdock'

MAPR_POST: &MAPR_POST_DEFAULTS
    start++:
        - echo "Waiting 1 minute for cluster services to stabilize ..."
        - sleep 60

POSTGRESQL_PRE: &POSTGRESQL_PRE_DEFAULTS
    args:
        --postgresql-user:
            default: postgres
            help: PostgreSQL superuser
            metavar: user
        --postgresql-password:
            default: postgres
            help: Superuser password for PostgreSQL
            metavar: password
        --postgresql-database:
            default: default
            help: Default database to create
            metavar: database
    start_help: Set GITHUB_USERNAME and GITHUB_TOKEN env variables
        before starting this environment
    start:
        - curl -u $GITHUB_USERNAME:$GITHUB_TOKEN
            -s https://raw.githubusercontent.com/streamsets/testenvironments/master/streamsets/testenvironments/scripts/setup_wal2json.sh
            -o {testenvironments-config-directory}/{environment}/setup_wal2json.sh
        - curl -u $GITHUB_USERNAME:$GITHUB_TOKEN
            -s https://raw.githubusercontent.com/streamsets/testenvironments/master/streamsets/testenvironments/resources/pg_hba.conf
            -o {testenvironments-config-directory}/{environment}/pg_hba.conf
        - curl -u $GITHUB_USERNAME:$GITHUB_TOKEN
            -s https://raw.githubusercontent.com/streamsets/testenvironments/master/streamsets/testenvironments/resources/postgresql.conf
            -o {testenvironments-config-directory}/{environment}/postgresql.conf
        # Set the permissions for script correctly as after cURL downloads it, no umask happens.
        - chmod u+x {testenvironments-config-directory}/{environment}/setup_wal2json.sh

POSTGRESQL_POST: &POSTGRESQL_POST_DEFAULTS
    stf arguments+:
        - --database-password {postgresql-password}
        - --database-username {postgresql-user}

CLUSTERDOCK_POST: &CLUSTERDOCK_POST_DEFAULTS
    stop:
        - '{testenvironments-config-directory}/{environment}/workspace/venv/bin/python
            {testenvironments-config-directory}/{environment}/workspace/venv/bin/clusterdock
            --clusterdock-config-directory {testenvironments-config-directory}/{environment} -v manage nuke'

environment_start:
    - mkdir -p {testenvironments-config-directory}/{environment}/workspace

environment_stop:
    - '{SUPER_COMMAND} rm -rf {testenvironments-config-directory}/{environment}'

environments:
    AWS_EMR_5.13.0:
        <<: *AWS_EMR_PRE_DEFAULTS
        start:
            - "{AWS_CLI_PREFIX_COMMAND} -e AWS_DEFAULT_REGION={aws-region} mesosphere/aws-cli $*
                emr create-cluster --applications Name=Hadoop --name 'stf-jenkins'
                --service-role EMR_DefaultRole --release-label emr-5.13.0
                --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole,SubnetIds=['{aws-ec2-subnet-id}'],EmrManagedMasterSecurityGroup={aws-ec2-security-group},EmrManagedSlaveSecurityGroup={aws-ec2-security-group}
                --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.xlarge
                InstanceGroupType=CORE,InstanceCount=1,InstanceType=m4.xlarge >
                {testenvironments-config-directory}/{environment}/clusterid.txt {extra-arguments}"
        <<: *AWS_EMR_POST_DEFAULTS
        stf arguments:
            - --aws-emr-cluster-id $(cat {testenvironments-config-directory}/{environment}/clusterid.txt)
    Broker:
        args:
            --mqtt-container-name:
                default: mymqtt
                help: MQTT Docker container name to use
                metavar: name
            --mqtt-image-name:
                default: toke/mosquitto
                help: MQTT Docker images to use
                metavar: name
            --rabbitmq-container-name:
                default: myrabbitmq
                metavar: name
            --rabbitmq-image-name:
                default: rabbitmq
                help: RabbitMQ Docker image to use
                metavar: name
            --rabbitmq-image-version:
                default: 3.5.6
                help: RabbitMQ Docker image version to use
                metavar: ver
        start:
            - docker run -d --net={docker-network} --name={mqtt-container-name} {mqtt-image-name}
            - docker run -d --net={docker-network} --name={rabbitmq-container-name}
                {rabbitmq-image-name}:{rabbitmq-image-version}
        stop:
            - docker rm -f {mqtt-container-name} {rabbitmq-container-name}
        stf arguments:
            - --mqtt-broker {mqtt-container-name}
            - --rabbitmq-url amqp://guest:guest@{rabbitmq-container-name}.{docker-network}:5672/%2F
    Cassandra_3.11.0:
        args:
            --container-name:
                default: mycassandra
                help: Cassandra Docker container name to use
                metavar: name
        start:
            - docker run -d --net={docker-network} --name {container-name} cassandra:3.11.0 {extra-arguments}
        stop:
            - docker rm -f {container-name}
        stf arguments:
            - --cassandra-contacts {container-name}
    CDH_5.9.0:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.9.0 --cm-version 5.9.0 --kafka-version 2.0.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.9.0_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS} --cdh-version 5.9.0 --cm-version 5.9.0
                --kafka-version 2.0.0 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.9.0_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.9.0 --cm-version 5.9.0 --kafka-version 2.0.0 --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.9.0_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.9.0 --cm-version 5.9.0 --kafka-version 2.0.0 --spark2-version 2.1-r1
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.9.0_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.9.0 --cm-version 5.9.0 --kafka-version 2.0.0
                --spark2-version 2.1-r1 --ssl encryption {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.9.0_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.9.0 --cm-version 5.9.0 --kafka-version 2.0.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.9.0_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.9.0 --cm-version 5.9.0 --kafka-version 2.0.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0
                --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0
                --spark2-version 2.1-r1 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0
                --spark2-version 2.1-r1 --ssl encryption {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.10.0_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.10.0 --cm-version 5.10.0 --kafka-version 2.1.0 --kudu-version 1.2.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1 --kafka-version 2.1.0 --kudu-version 1.3.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1 --kafka-version 2.1.0 --kudu-version 1.3.0
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1 --kafka-version 2.1.0 --kudu-version 1.3.0
                --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1 --kafka-version 2.1.0 --kudu-version 1.3.0
                --spark2-version 2.1-r1 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1
                --kafka-version 2.1.0 --kudu-version 1.3.0 --spark2-version 2.1-r1 --ssl encryption
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1 --kafka-version 2.1.0 --kudu-version 1.3.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.11.1_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.11.1 --cm-version 5.11.1 --kafka-version 2.1.0 --kudu-version 1.3.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0
                --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0
                --spark2-version 2.1-r1 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0
                --spark2-version 2.1-r1 --ssl encryption {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.12.0_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.12.0 --cm-version 5.12.0 --kafka-version 2.1.0 --kudu-version 1.4.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0
                --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0
                --spark2-version 2.1-r1 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0
                --spark2-version 2.1-r1 --ssl encryption {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.13.0_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.13.0 --cm-version 5.13.0 --kafka-version 3.0.0 --kudu-version 1.5.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0
                --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0
                --spark2-version 2.1-r1 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0
                --spark2-version 2.1-r1 --ssl encryption {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.14.0_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.14.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.6.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.1.0 --kudu-version 1.7.0 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.7.0
                {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0_Spark2:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.7.0
                --spark2-version 2.1-r1 {extra-arguments}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0_Spark2_Kerberos:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.7.0
                --spark2-version 2.1-r1 {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0_Spark2_Kerberos_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.7.0
                --spark2-version 2.1-r1 --ssl encryption {CLUSTERDOCK_KERBEROS_ARGS} {extra-arguments}'
            - '{CLUSTERDOCK_KERBEROS_COPY}'
            - '{CLUSTERDOCK_KERBEROS_KEYTAB_RENAME}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_KERBEROS_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0_Spark2_SSL_Auth:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.7.0
                --spark2-version 2.1-r1 --ssl authentication {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    CDH_5.15.0_Spark2_SSL_Encrypt:
        <<: *CDH_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_CDH_ARGS}
                --cdh-version 5.15.0 --cm-version 5.15.0 --kafka-version 3.0.0 --kudu-version 1.7.0
                --spark2-version 2.1-r1 --ssl encryption {extra-arguments}'
            - '{CLUSTERDOCK_SSL_KEYS_CERTS_COPY}'
        <<: *CDH_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    Couchbase_5.1.1:
        start:
            - docker run -d --name=mycouchbase --network={docker-network} {extra-arguments} couchbase:enterprise-5.1.1
            - sleep 3
            - docker exec mycouchbase bash -c "for i in {{1..120}}; do couchbase-cli server-info -c=localhost
                -u={COUCHBASE_DEFAULT_USERNAME} -p={COUCHBASE_DEFAULT_PASSWORD} >/dev/null && exit 0 || sleep 1;
                done; echo 'Cannot connect to Couchbase'; exit 1"
            - docker exec mycouchbase couchbase-cli cluster-init --cluster-username={COUCHBASE_DEFAULT_USERNAME}
                --cluster-password={COUCHBASE_DEFAULT_PASSWORD} --services=data,index,query,fts --cluster-ramsize=256
                --cluster-index-ramsize=256 --cluster-fts-ramsize=256 --index-storage-setting=default
        stop:
            - docker rm -f mycouchbase
        stf arguments:
            - --couchbase-uri couchbase://mycouchbase:8091
    DSE_5.1.3:
        <<: *DSE_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_DSE_ARGS} --dse-version 5.1.3-1 {extra-arguments}'
            - '{SUPER_COMMAND} mv -f {testenvironments-config-directory}/{environment}/clusterdock.keytab
                {testenvironments-config-directory}/{environment}/sdc.keytab'
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cassandra-contacts node-1.{docker-network} node-2.{docker-network}
            - --cassandra-kerberos
            - --kerberos
    Elasticsearch_5.2.0:
        start:
            - docker run -d --name=myelastic --net={docker-network}
                -e "http.host=0.0.0.0" -e "transport.host=127.0.0.1" {extra-arguments}
                docker.elastic.co/elasticsearch/elasticsearch:5.2.0
        stop:
            - docker rm -f myelastic
        stf arguments:
            - --elasticsearch-url http://elastic:changeme@myelastic.{docker-network}:9200
    ELS_MDB_Solr_Http:
        # TODOs:
        # 1. migrate the mongodb_replica_setup.js script to testenvironments github repo (once after its setup)
        # 2. remove GITHUB_USERNAME and GITHUB_TOKEN dependency once after STE goes public repo
        start_help: Set GITHUB_USERNAME and GITHUB_TOKEN env variables before starting this environment
        start:
            - curl -u $GITHUB_USERNAME:$GITHUB_TOKEN
                -s https://raw.githubusercontent.com/streamsets/infra/master/testframework/jenkins/scripts/mongodb_replica_setup.js
                -o {testenvironments-config-directory}/{environment}/mongodb_replica_setup.js
            - docker run -d --name=myelastic --net={docker-network}
                -e "http.host=0.0.0.0" -e "transport.host=127.0.0.1"
                docker.elastic.co/elasticsearch/elasticsearch:5.2.0
            - docker run -d --name=mysolr --net={docker-network}
                -t solr:6.1.0 solr-precreate mycore
            - docker run -d --name myhttpmockserver --net={docker-network} pretenders/pretenders:1.4
            - docker run -d --name=mymongo1 --net={docker-network}
                -v {testenvironments-config-directory}/{environment}:/root/scripts mongo mongod --replSet my-mongo-set
            - docker run -d --name=mymongo2 --net={docker-network} mongo mongod --replSet my-mongo-set
            - docker run -d --name=mymongo3 --net={docker-network} mongo mongod --replSet my-mongo-set
            - sleep 3
            - docker exec -i mymongo1 mongo /root/scripts/mongodb_replica_setup.js
        stop:
            - docker rm -f myelastic mysolr myhttpmockserver mymongo1 mymongo2 mymongo3
        stf arguments:
            - --elasticsearch-url http://elastic:changeme@myelastic.{docker-network}:9200
            - --mongodb-uri mongodb://mymongo1:27017/?replicaSet=my-mongo-set
            - --solr-uri http://mysolr:8983/solr/mycore
            - --http-server-url http://myhttpmockserver:8000
    GCP:
        start_help: Set GCP_KEY env variable before starting this environment
        start:
            - echo $GCP_KEY > {testenvironments-config-directory}/{environment}/GCP-credentials.json
            - echo "stf-`head /dev/urandom | tr -dc a-z0-9 | head -c 10`" >
                   {testenvironments-config-directory}/{environment}/bigtable_instance.txt
            - docker run --rm
                -e GCP_BIGTABLE_INSTANCE=$(cat {testenvironments-config-directory}/{environment}/bigtable_instance.txt)
                -v {testenvironments-config-directory}/{environment}:/root/config google/cloud-sdk:slim
                bash -c '{GCP_ENV_CONFIG_COMMANDS} && {GCP_BIGTABLE_INSTANCE_CREATE_COMMAND}'
        stop:
            - docker run --rm
                -e GCP_BIGTABLE_INSTANCE=$(cat {testenvironments-config-directory}/{environment}/bigtable_instance.txt)
                -v {testenvironments-config-directory}/{environment}:/root/config google/cloud-sdk:slim
                bash -c '{GCP_ENV_CONFIG_COMMANDS} && {GCP_BIGTABLE_INSTANCE_DELETE_COMMAND}'
        stf arguments:
            - --gcp-credentials-filename GCP-credentials.json
            - --gcp-project-name StreamSets-Engineering
            - --gcp-bigtable-instance-name {testenvironments-config-directory}/{environment}/bigtable_instance.txt
    HDP_2.6.4.0:
        <<: *HDP_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_HDP_ARGS} --hdp-version 2.6.4.0 --ambari-version 2.6.1.0
                {extra-arguments}'
        <<: *HDP_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
    HTTP:
        start:
            - docker run -d --name myhttpmockserver --net={docker-network} {extra-arguments} pretenders/pretenders:1.4
        stop:
            - docker rm -f myhttpmockserver
        stf arguments:
            - --http-server-url http://myhttpmockserver:8000
    InfluxDB_0.13:
        start:
            - docker run -d --net={docker-network} --name=myinfluxdb
                -e INFLUXDB_USER=sdc -e INFLUXDB_USER_PASSWORD=sdc {extra-arguments} influxdb:0.13
        stop:
            - docker rm -f myinfluxdb
        stf arguments:
            - --influxdb-uri influxdb://sdc:sdc@myinfluxdb.{docker-network}:8086/testdb
    JMS_ActiveMQ:
        start:
            - docker run --name='activemq' -it --rm --net={docker-network} {extra-arguments}
                 -e 'ACTIVEMQ_MIN_MEMORY=512' -e 'ACTIVEMQ_MAX_MEMORY=2048' -dP webcenter/activemq:5.14.3
        stop:
            - docker rm -f activemq
        stf arguments:
            - --jms-server-url tcp://activemq:61616
            - --jms-client-port 61613
    Kafka_0.9:
        <<: *KAFKA_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_KAFKA_ARGS} --kafka-version 0.9.0.1 {extra-arguments}'
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_SCHEMA_REGISTRY_ARGS}'
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server kafka://node-1.{docker-network}:9092,node-2.{docker-network}:9092,node-3.{docker-network}:9092
            - --kafka-version 0.9.0.1
            - --kafka-zookeeper node-1.{docker-network}:2181,node-2.{docker-network}:2181,node-3.{docker-network}:2181
            - --confluent-schema-registry http://registry-1.{docker-network}:8081
    Kafka_0.10:
        <<: *KAFKA_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_KAFKA_ARGS} --kafka-version 0.10.0.1 {extra-arguments}'
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_SCHEMA_REGISTRY_ARGS}'
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server kafka://node-1.{docker-network}:9092,node-2.{docker-network}:9092,node-3.{docker-network}:9092
            - --kafka-version 0.10.0.1
            - --kafka-zookeeper node-1.{docker-network}:2181,node-2.{docker-network}:2181,node-3.{docker-network}:2181
            - --confluent-schema-registry http://registry-1.{docker-network}:8081
    Kafka_0.11:
        <<: *KAFKA_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_KAFKA_ARGS} --kafka-version 0.11.0.1 {extra-arguments}'
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_SCHEMA_REGISTRY_ARGS}'
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server kafka://node-1.{docker-network}:9092,node-2.{docker-network}:9092,node-3.{docker-network}:9092
            - --kafka-version 0.11.0.1
            - --kafka-zookeeper node-1.{docker-network}:2181,node-2.{docker-network}:2181,node-3.{docker-network}:2181
            - --confluent-schema-registry http://registry-1.{docker-network}:8081
    Kafka_1.0:
        <<: *KAFKA_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_KAFKA_ARGS} --kafka-version 1.0.0 {extra-arguments}'
            - '{CLUSTERDOCK_START_ARGS} {CLUSTERDOCK_SCHEMA_REGISTRY_ARGS}'
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server kafka://node-1.{docker-network}:9092,node-2.{docker-network}:9092,node-3.{docker-network}:9092
            - --kafka-version 1.0.0
            - --kafka-zookeeper node-1.{docker-network}:2181,node-2.{docker-network}:2181,node-3.{docker-network}:2181
            - --confluent-schema-registry http://registry-1.{docker-network}:8081
    MapR_5.2.0:
        <<: *MAPR_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_MAPR_ARGS} --mapr-version 5.2.0 {extra-arguments}'
        <<: *MAPR_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server mapr://https://node-1.{docker-network}:8443
    MapR_5.2.2_MEP_1.1.3:
        <<: *MAPR_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_MAPR_ARGS} --mapr-version 5.2.2 --mep-version 1.1.3
                {extra-arguments}'
        <<: *MAPR_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server mapr://https://node-1.{docker-network}:8443?mep=1.1.3
    MapR_5.2.2_MEP_3.0.1:
        <<: *MAPR_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_MAPR_ARGS} --mapr-version 5.2.2 --mep-version 3.0.1
                {extra-arguments}'
        <<: *MAPR_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server mapr://https://node-1.{docker-network}:8443?mep=3.0.1
    MapR_6.0.0_MEP_4.0:
        <<: *MAPR_PRE_DEFAULTS
        start+:
            - '{CLUSTERDOCK_START_ARGS_SS} {CLUSTERDOCK_MAPR_ARGS}
                --mapr-version 6.0.0 --mep-version 4.0
                --license-url http://stage.mapr.com/license/LatestDemoLicense-M7.txt
                --license-credentials streamsets:mapr4streamsets {extra-arguments}'
        <<: *MAPR_POST_DEFAULTS
        <<: *CLUSTERDOCK_POST_DEFAULTS
        stf arguments:
            - --cluster-server mapr://https://node-1.{docker-network}:8443?mep=4.0
    MongoDB:
        # TODOs:
        # 1. migrate the mongodb_replica_setup.js script to testenvironments github repo (once after its setup)
        # 2. remove GITHUB_USERNAME and GITHUB_TOKEN dependency once after STE goes public repo
        start_help: Set GITHUB_USERNAME and GITHUB_TOKEN env variables before starting this environment
        start:
            - curl -u $GITHUB_USERNAME:$GITHUB_TOKEN
                -s https://raw.githubusercontent.com/streamsets/infra/master/testframework/jenkins/scripts/mongodb_replica_setup.js
                -o {testenvironments-config-directory}/{environment}/mongodb_replica_setup.js
            - docker run -d --name=mymongo1 --net={docker-network}
                -v {testenvironments-config-directory}/{environment}:/root/scripts mongo mongod --replSet my-mongo-set
            - docker run -d --name=mymongo2 --net={docker-network} mongo mongod --replSet my-mongo-set
            - docker run -d --name=mymongo3 --net={docker-network} mongo mongod --replSet my-mongo-set
            - sleep 3
            - docker exec -i mymongo1 mongo /root/scripts/mongodb_replica_setup.js
        stop:
            - docker rm -f mymongo1 mymongo2 mymongo3
        stf arguments:
            - --mongodb-uri mongodb://mymongo1:27017/?replicaSet=my-mongo-set
    MQTT:
        start:
            - docker run -d --net={docker-network} --name=mymqtt {extra-arguments} toke/mosquitto
        stop:
            - docker rm -f mymqtt
        stf arguments:
            - --mqtt-broker mymqtt
    MySQL_5.7:
        start:
            - docker run -d --net={docker-network} --name=mysql -e MYSQL_ROOT_PASSWORD=root
                -e MYSQL_DATABASE=default -e MYSQL_USER=mysql -e MYSQL_PASSWORD=mysql {extra-arguments}
                mysql:5.7
            - echo "Waiting 1 minute for database to stabilize ..."
            - sleep 60
        stop:
            - docker rm -f mysql
        stf arguments:
            - --database mysql://mysql.{docker-network}:3306/default
    PostgreSQL_9.6.2:
        start:
            - docker run -d --net={docker-network} --name=postgres
                -e POSTGRES_DB=default {extra-arguments} postgres:9.6.2-alpine
        stop:
            - docker rm -f postgres
        stf arguments:
            - --database postgresql://postgres.{docker-network}:5432/default
    PostgreSQL_CDC_9.4.18:
        <<: *POSTGRESQL_PRE_DEFAULTS
        start+:
            - 'docker run -d --net={docker-network} --name=postgres-cdc-9.4.18
                -v {testenvironments-config-directory}/{environment}:/etc/postgresql
                -e POSTGRES_USER={postgresql-user} -e POSTGRES_PASSWORD={postgresql-password}
                -e POSTGRES_DB={postgresql-database} {extra-arguments} postgres:9.4.18
                {POSTGRESQL_CONFIG_FILES_COMMAND}'
            - docker exec -i postgres-cdc-9.4.18 bash -c '/etc/postgresql/setup_wal2json.sh 9.4'
        stop:
            - docker rm -f -v postgres-cdc-9.4.18
        stf arguments:
            - --database postgresql://postgres-cdc-9.4.18.{docker-network}:5432/{postgresql-database}
        <<: *POSTGRESQL_POST_DEFAULTS
    PostgreSQL_CDC_9.6.9:
        <<: *POSTGRESQL_PRE_DEFAULTS
        start+:
            - 'docker run -d --net={docker-network} --name=postgres-cdc-9.6.9
                -v {testenvironments-config-directory}/{environment}:/etc/postgresql
                -e POSTGRES_USER={postgresql-user} -e POSTGRES_PASSWORD={postgresql-password}
                -e POSTGRES_DB={postgresql-database} {extra-arguments} postgres:9.6.9
                {POSTGRESQL_CONFIG_FILES_COMMAND}'
            - docker exec -i postgres-cdc-9.6.9 bash -c '/etc/postgresql/setup_wal2json.sh 9.6'
        stop:
            - docker rm -f -v postgres-cdc-9.6.9
        stf arguments:
            - --database postgresql://postgres-cdc-9.6.9.{docker-network}:5432/{postgresql-database}
        <<: *POSTGRESQL_POST_DEFAULTS
    PostgreSQL_CDC_10.4:
        <<: *POSTGRESQL_PRE_DEFAULTS
        start+:
            - 'docker run -d --net={docker-network} --name=postgres-cdc-10.4
                -v {testenvironments-config-directory}/{environment}:/etc/postgresql
                -e POSTGRES_USER={postgresql-user} -e POSTGRES_PASSWORD={postgresql-password}
                -e POSTGRES_DB={postgresql-database} {extra-arguments} postgres:10.4
                {POSTGRESQL_CONFIG_FILES_COMMAND}'
            - docker exec -i postgres-cdc-10.4 bash -c '/etc/postgresql/setup_wal2json.sh 10'
        stop:
            - docker rm -f -v postgres-cdc-10.4
        stf arguments:
            - --database postgresql://postgres-cdc-10.4.{docker-network}:5432/{postgresql-database}
        <<: *POSTGRESQL_POST_DEFAULTS
    RabbitMQ_3.5.6:
        start:
            - docker run -d --net={docker-network} --name=myrabbitmq {extra-arguments} rabbitmq:3.5.6
        stop:
            - docker rm -f myrabbitmq
        stf arguments:
            - --rabbitmq-url amqp://guest:guest@myrabbitmq.{docker-network}:5672/%2F
    Redis_4.0.1:
        start:
            - docker run -d --net={docker-network} --name=myredis {extra-arguments} redis:4.0.1
        stop:
            - docker rm -f myredis
        stf arguments:
            - --redis-uri redis://myredis.{docker-network}:6379/0
    Redis_Cassandra:
        start:
            - docker run -d --net={docker-network} --name=myredis redis:4.0.1
            - docker run -d --net={docker-network} --name mycassandra cassandra:3.11.0
        stop:
            - docker rm -f myredis
            - docker rm -f mycassandra
        stf arguments:
            - --redis-uri redis://myredis.{docker-network}:6379/0
            - --cassandra-contacts mycassandra
    Solr_6.1.0:
        start:
            - docker run -d --name=mysolr --net={docker-network} {extra-arguments} -t solr:6.1.0 solr-precreate mycore
        stop:
            - docker rm -f mysolr
        stf arguments:
            - --solr-uri http://mysolr:8983/solr/mycore
