{"who":{"user":{"email_address":null,"enabled":1,"id":3001865,"login_name":"kdreyer@redhat.com","orgchart_id":null,"preferences":{"default_filter_id":"1039","show_full_info":"1","color_scheme":""},"realname":"Ken Dreyer","receives_mail":true,"user_organization_id":142}},"params":{"format":"json","action":"show","controller":"api/v1/erratum","id":"26175"},"errata":{"rhba":{"actual_ship_date":"2017-03-14T15:41:16Z","assigned_to_id":3002003,"batch_id":null,"closed":0,"content_types":["rpm"],"contract":null,"created_at":"2017-01-10T23:16:02Z","current_state_index_id":158326,"current_tps_run":null,"deleted":0,"devel_responsibility_id":3,"doc_complete":1,"docs_responsibility_id":1,"embargo_undated":false,"filelist_changed":0,"filelist_locked":0,"fulladvisory":"RHBA-2017:0514-04","group_id":655,"id":26175,"is_batch_blocker":false,"is_brew":1,"is_valid":1,"issue_date":"2017-03-14T15:30:26Z","mailed":0,"manager_id":3001931,"old_advisory":"RHBA-2017:26175-04","old_delete_product":null,"package_owner_id":3001865,"priority":"normal","product_id":104,"publish_date_override":null,"published":1,"published_shadow":0,"pushcount":3,"pushed":1,"qa_complete":1,"quality_responsibility_id":2,"rating":0,"release_date":null,"reporter_id":3001865,"request":0,"request_rcm_push_comment_id":null,"resolution":"","respin_count":2,"revision":4,"rhn_complete":0,"rhnqa":1,"rhnqa_shadow":0,"security_approved":null,"security_impact":"None","security_sla":null,"severity":"normal","sign_requested":0,"state_machine_rule_set_id":null,"status":"SHIPPED_LIVE","status_updated_at":"2017-03-21T03:24:26Z","supports_multiple_product_destinations":true,"synopsis":"Red Hat Ceph Storage 2.2 bug fix and enhancement update","text_only":false,"text_ready":0,"update_date":"2017-03-17T22:07:06Z","updated_at":"2017-05-22T23:14:00Z","errata_id":514}},"content":{"content":{"crossref":"","cve":"","description":"Red Hat Ceph Storage is a scalable, open, software-defined storage platform\nthat combines the most stable version of the Ceph storage system with a\nCeph management platform, deployment utilities, and support services.\n\nBug Fixes and Enhancements:\n\nFor detailed information on changes in this release, see the Red Hat Ceph\nStorage 2.2 Release Notes available at:\n\nhttps://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2.2/html-single/release_notes/","doc_review_due_at":null,"doc_reviewer_id":3002718,"errata_id":26175,"how_to_test":null,"id":23753,"keywords":"","multilib":null,"obsoletes":"","packages":null,"product_version_text":null,"reference":"","revision_count":1,"solution":"Before applying this update, make sure all previously released errata\nrelevant to your system have been applied.\n\nFor details on how to apply this update, refer to:\n\nhttps://access.redhat.com/articles/11258","text_only_cpe":null,"topic":"Red Hat Ceph Storage 2.2 is now available.","updated_at":"2017-02-24T16:11:13Z"}},"diffs":{},"bugs":{"errata":{"rhba":{"actual_ship_date":"2017-03-14T15:41:16Z","assigned_to_id":3002003,"batch_id":null,"closed":0,"content_types":["rpm"],"contract":null,"created_at":"2017-01-10T23:16:02Z","current_state_index_id":158326,"current_tps_run":null,"deleted":0,"devel_responsibility_id":3,"doc_complete":1,"docs_responsibility_id":1,"embargo_undated":false,"filelist_changed":0,"filelist_locked":0,"fulladvisory":"RHBA-2017:0514-04","group_id":655,"id":26175,"is_batch_blocker":false,"is_brew":1,"is_valid":1,"issue_date":"2017-03-14T15:30:26Z","mailed":0,"manager_id":3001931,"old_advisory":"RHBA-2017:26175-04","old_delete_product":null,"package_owner_id":3001865,"priority":"normal","product_id":104,"publish_date_override":null,"published":1,"published_shadow":0,"pushcount":3,"pushed":1,"qa_complete":1,"quality_responsibility_id":2,"rating":0,"release_date":null,"reporter_id":3001865,"request":0,"request_rcm_push_comment_id":null,"resolution":"","respin_count":2,"revision":4,"rhn_complete":0,"rhnqa":1,"rhnqa_shadow":0,"security_approved":null,"security_impact":"None","security_sla":null,"severity":"normal","sign_requested":0,"state_machine_rule_set_id":null,"status":"SHIPPED_LIVE","status_updated_at":"2017-03-21T03:24:26Z","supports_multiple_product_destinations":true,"synopsis":"Red Hat Ceph Storage 2.2 bug fix and enhancement update","text_only":false,"text_ready":0,"update_date":"2017-03-17T22:07:06Z","updated_at":"2017-05-22T23:14:00Z","errata_id":514}},"id_field":"id","id_prefix":"bz:","type":"bugs","idsfixed":["1250713","1252600","1258961","1262117","1298571","1330952","1333667","1333809","1358020","1359403","1359404","1360674","1363689","1368508","1369405","1371212","1375898","1380177","1382226","1385485","1387797","1389159","1390336","1390716","1391197","1391250","1394495","1394928","1399624","1400915","1401906","1401936","1402076","1402080","1404419","1408226","1410921","1414324","1414613","1415260","1415640","1415981","1416160","1417178","1418201","1418809","1420231","1420530","1420675","1420793","1421099","1422059","1422379","1422875","1423417","1424687","1424725","1424881","1425710","1425763","1425771"],"bugs":[{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1250713,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-27T20:02:52Z","package_id":1990,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-28T04:02:47Z","release_notes":"","short_desc":"config_tempest.py breaks if Rados Gateway is configured for object-store","verified":"Any, FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1252600,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:57:31Z","package_id":39676,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T15:57:54Z","release_notes":".The Ceph Object Gateway now passes all Swift tests in the RefSTack Tempest test suite version 10.0.0-3\r\n\r\nPreviously, the Ceph Object Gateway failed certain RefSTack Tempest tests, such as the TempURL and object versioning tests. With this update, the underlying source code has been modified, and the Ceph Object Gateway now correctly passes all tests. \r\n\r\nIn addition, to pass the \"(0) content-length header after object deletion present\" test, set the `rgw print prohibited content length` setting in the Ceph configuration file to `true`.\r\n\r\nIf the Ceph Object Gateway is configured for Object Store and not for Swift, perform the following steps to pass the tests:\r\n\r\n. During the Tempest configuration, set the following parameters in the Ceph configuration file:\r\n+\r\n----\r\nrgw_swift_url_prefix = \"/\"\r\nrgw_enable_apis=swift, swift_auth, admin\r\n----\r\n\r\n. Once the configuration is complete, comment the parameters out:\r\n+\r\n----\r\n# rgw_swift_url_prefix\r\n# rgw_enable_apis\r\n----\r\n\r\nSee the https://access.redhat.com/solutions/2953751[config_tempest.py breaks if Rados Gateway is configured for object-store] solution for details.","short_desc":"Ensure RGW passes all upstream  Swift Tempest tests in RefStack toolset","verified":"Any, FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,needinfo?,pm_ack+,qa_ack+,requires_doc_text+","id":1258961,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature, Triaged","last_updated":"2017-07-30T16:01:21Z","package_id":39676,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T16:02:30Z","release_notes":".Support for the SSL protocol has been added\r\n\r\nThe Ceph Object Gateway now supports the SSL protocol. Previously, a reverse proxy server with SSL had to be set up to dispatch HTTPS requests. For details, see the https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/object_gateway_guide_for_red_hat_enterprise_linux/#using_ssl_with_civetweb[Using SSL with Civetweb] chapter in the Ceph Object Gateway Guide.","short_desc":"[RFE] Enable SSL support in RHCS RadosGW","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1262117,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2017-07-30T15:54:33Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:54:53Z","release_notes":".The Ceph Object Gateway now supports the Swift object versioning\r\n\r\nThe Ceph Object Gateway now supports the Swift object versioning APIs, including correct handling of the `X-Versions-Location` header. The `X-History-Location` header is not supported.\r\n\r\nThe object versioning is an object-native version control mechanism of Swift object storage, and is a required capability for RefStack conformance. For details, see the https://docs.openstack.org/developer/swift/overview_object_versioning.html[Object Versioning] section of the OpenStack documentation.","short_desc":"[RFE] Swift Object Versioning support","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1298571,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:47:47Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:48:51Z","release_notes":".The Ceph Object Gateway now supports custom HTTP header logging\r\n\r\nSites that use the Civetweb HTTP web server previously lacked the ability to log custom HTTP headers, as they could when using the Apache web server and the FastCGI protocol. With this update, the Ceph Object Gateway supports custom HTTP header logging. \r\n\r\nTo log custom HTTP headers, enable the operations log socket on the Ceph Object Gateway instance and list the HTTP headers. Add the following parameters to the Ceph configuration file:\r\n\r\n----\r\nrgw enable ops log = true\r\nrgw ops log socket path = <path>\r\nrgw log http headers = \"<headers>\"\r\n----\r\n\r\nReplace `<path>` with the path to the operations log socket and `<headers` with a comma-separated list of custom HTTP headers, for example:\r\n\r\n----\r\nrgw enable ops log = true\r\nrgw ops log socket path = /tmp/opslog\r\nrgw log http headers = \"http_x_forwarded_for, http_expect, http_content_md5\"\r\n----\r\n\r\nThe operations log stream then lists the headers as a JSON-formatted key-value list with the \"http_x_headers\" key.","short_desc":"[RFE] Log X-Forwarded-For information","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1330952,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:39:54Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:41:17Z","release_notes":".Multi-site configuration of the Ceph Object Gateway sometimes fails when options are changed at runtime\r\n\r\nWhen the `rgw md log max shards` and `rgw data log num shards` options are changed at runtime in multi-site configuration of the Ceph Object Gateway, the `radosgw` process terminates unexpectedly with a segmentation fault.\r\n\r\nTo avoid this issue, do not change the aforementioned options at runtime, but set them during the initial configuration of the Ceph Object Gateway.","short_desc":"radosgw process segfaults when multisite params are changed w/sync in progress","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text-","id":1333667,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:43:59Z","package_id":29771,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"undefined","short_desc":"2606 salt-minion processes (fork bomb) after ceph package upgrade to Red Hat Ceph Storage 1.3.2","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1333809,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature, Triaged","last_updated":"2017-07-30T15:16:58Z","package_id":39674,"pm_score":0,"priority":"medium","qa_whiteboard":"CEPH-10839","reconciled_at":"2017-07-30T15:17:33Z","release_notes":".OSD heartbeat_check log messages now include IP addresses\r\n\r\nThe OSD `heartbeat_check` log messages now include IP addresses of the OSD nodes. This enhancement improves identification of the OSD nodes in the Ceph logs. For example, it is no longer necessary to look up which IP correlates to which OSD node (OSD.<number>) for the `heartbeat_check` message in the log.","short_desc":"[RFE] Improve OSD heartbeat_check log message by including host name or IP (besides OSD numbers)","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1358020,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:51:30Z","package_id":39676,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2017-07-30T15:51:51Z","release_notes":".Setting file permissions and ownership attributes no longer fails on existing files and directories\r\n\r\nPreviously, the NFS Ganesha file system failed to serialize and store UNIX attributes on existing files and directories. Consequently, file permissions and ownership attributes that were set after file or directory creation were not correctly stored. The underlying source code has been modified, and setting file permissions and ownership attributes no longer fails on existing files and directories.","short_desc":"rgw_setattr does not change attrs","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1359403,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:56:34Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:57:52Z","release_notes":"","short_desc":"[RGW-NFS]:- objects created from s3 apis are not visible from nfs mount point","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1359404,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:48:16Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:48:51Z","release_notes":".Buckets no longer have incorrect time stamps\r\n\r\nPreviously, buckets created by the Simple Storage Service (S3) API on the Ceph Object Gateway before mounting the Ganesha NFS interface had incorrect time stamps when viewed from NFS. With this update, the NFS service uses time stamps that are based on the correct times of creation or modification of buckets. As a result, buckets created by the S3 API no longer have incorrect time stamps.","short_desc":"[RGW:NFS]:- Timestamp for directories (rgw buckets) which were created before ganesha instantiation is too old","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1360674,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:41:28Z","package_id":39676,"pm_score":0,"priority":"low","qa_whiteboard":"","reconciled_at":"2017-07-30T15:42:45Z","release_notes":"","short_desc":"[RGW:NFS]:- Every subdirectory access from the mount will have separate mount points on the client side","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1363689,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:47:27Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:48:51Z","release_notes":".Restart of the `radosgw` service on clients is no longer needed after rebooting the cluster\r\n\r\nPreviously, after rebooting the Ceph cluster, it was necessary to restart the `radosgw` service on the Ceph Object Gateway clients to restore the connection with the cluster. With this update, the restart of `radosgw` is no longer needed.","short_desc":"[RGW] After a cluster reboot, a rgw service restart is needed on the clients","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1368508,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:46:04Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"CEPH-10836","reconciled_at":"2017-07-30T15:47:22Z","release_notes":"","short_desc":"ganesha.nfsd thread segfaults when keyring is not specified in client.rgw section of ceph.conf file","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1369405,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2017-07-30T15:51:03Z","package_id":39676,"pm_score":0,"priority":"medium","qa_whiteboard":"CEPH-10843","reconciled_at":"2017-07-30T15:51:49Z","release_notes":"","short_desc":"support custom search filter in ldap authentication","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1371212,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:58:24Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:59:28Z","release_notes":".The radosgw-admin orphan find command works as expected\r\n\r\nWhen listing objects, a segment marker caused incorrect listing of a subset of the Ceph Object Gateway internal objects. This behavior caused the `radosgw-admin orphan find` command to enter an infinite loop. This bug has been fixed, and the `radosgw-admin orphan find` command now works correctly.","short_desc":"radosgw-admin orphan find command doesn't complete","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1375898,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-06-06T04:54:00Z","package_id":18485,"pm_score":700,"priority":"high","qa_whiteboard":"gradeB TestOnlyCandidate BaseOSTriagedHigh_1256374","reconciled_at":"2017-06-06T04:55:04Z","release_notes":"","short_desc":"Compiling Ceph with lttng requires additional selinux policy changes","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1380177,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:20:34Z","package_id":39674,"pm_score":0,"priority":"low","qa_whiteboard":"CEPH-10841","reconciled_at":"2017-07-30T15:22:04Z","release_notes":"","short_desc":"Remove the Dependency on redhat-lsb-core Package from the ceph-base Package","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1382226,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:13:06Z","package_id":39674,"pm_score":0,"priority":"low","qa_whiteboard":"CEPH-10792","reconciled_at":"2017-07-30T15:14:30Z","release_notes":".osd_scrub_chunk_max is honored also with objects that have many clones\r\n\r\nPreviously, deep scrubbing of objects that had a number of clones could impact the client performance. With this enhancement, deep scrubbing honors the limit specified by the `osd_scrub_chunk_max` parameter even when an object has many clones. As a result, the impact on client performance is limited.","short_desc":"PG scrub bypasses 'osd_scrub_chunk_max' limit to find hash boundary","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1385485,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:53:01Z","package_id":39676,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T15:53:24Z","release_notes":"","short_desc":"404 ends in \\n instead of \\r\\n, causing errors in some libraries","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1387797,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:45:56Z","package_id":29771,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"","short_desc":"socket.getnameinfo returns an IP and calamari fails to list all participating OSD in cluster","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1389159,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Regression","last_updated":"2017-07-30T15:15:48Z","package_id":39674,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T15:16:01Z","release_notes":"","short_desc":"RHCS 2 daemons can not dump core because PR_SET_DUMPABLE is set to 0 after setuid call","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1390336,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:44:11Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:44:18Z","release_notes":"","short_desc":"rgw:  json encode/decode of RGWBucketInfo missing index_type field","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1390716,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:53:56Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"CEPH-10838","reconciled_at":"2017-07-30T15:54:53Z","release_notes":".Listing bucket info data no longer causes the OSD daemon to terminate unexpectedly\r\n\r\nDue to invalid memory access in an object class operation, the `radosgw-admin bi list --max-entries=1` command in some cases caused the Ceph OSD daemon to terminate unexpectedly with a segmentation fault. This bug has been fixed, and listing bucket info data no longer causes the OSD daemon to crash.","short_desc":"osd crashes when \"radosgw-admin bi list --max-entries=1\" command runing","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1391197,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-11-01T20:03:41Z","package_id":39672,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-11-01T20:05:13Z","release_notes":".OSD nodes no longer fail to start after a reboot\r\n\r\nWhen the `ceph-osd` service was enabled for a given OSD device, a race condition in some cases occurred between the `ceph-osd` and the `ceph-disk` services at boot time. As a consequence, the OSD did not start after a reboot. With this update, the `ceph-disk` utility now calls the `systemctl enable` and `disable` commands with the `--runtime` option so that the `ceph-osd` units are lost after a reboot. As result, OSD nodes start as expected after a reboot.","short_desc":"OSD's not starting after a reboot in RHCS 2.0","verified":"SanityOnly","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1391250,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:13:19Z","package_id":39674,"pm_score":0,"priority":"medium","qa_whiteboard":"CEPH-10787","reconciled_at":"2017-07-30T15:14:30Z","release_notes":".The \"ceph df\" output no longer includes OSD nodes marked as \"out\"\r\n\r\nThe `ceph df` command shows cluster free space. Previously, the OSD node that were marked as `out` were incorrectly included in the output of `ceph df`. Consequently, if the Ceph cluster included an OSD node that was marked as `out`, the output of `ceph df` was incorrect. This bug has been fixed, and `ceph df` now correctly reports cluster free space.","short_desc":"Backport: \"ceph osd df does not show summarized info correctly if one or more OSDs are out\" : http://tracker.ceph.com/issues/16706","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1394495,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:15:43Z","package_id":39674,"pm_score":0,"priority":"medium","qa_whiteboard":"CEPH-10790","reconciled_at":"2017-07-30T15:16:01Z","release_notes":".Removing a Ceph Monitor no longer returns an error\r\n\r\nWhen removing a Ceph Monitor by using the `ceph mon remove` command, the Monitor was successfully removed, but an error message similar to the following was returned:\r\n\r\n----\r\nError EINVAL: removing mon.magna072 at 10.8.128.72:6789/0, there will be 3 monitors\r\n----\r\n\r\nThe underlying source code has been fixed, and the error is no longer returned when removing Ceph Monitors.","short_desc":"Removing Mon with CLI - Error message getting displayed though cluster is not disrupted","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1394928,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:12:04Z","package_id":39674,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:12:58Z","release_notes":"","short_desc":"rolling upgrade on ubuntu can leave osd processes running, but not marked up","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1399624,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:42:21Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:42:47Z","release_notes":"","short_desc":"fix for deleting objects name beginning and ending with underscores of one bucket using POST method of js sdk","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1400915,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:30:18Z","package_id":39675,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T15:31:12Z","release_notes":".The rados utility now supports the --omap-key-file option\r\n\r\nWith this update, the `rados` command-line utility supports the `--omap-key-file` option. You can use this option to specify the path to a file containing the binary key for `omap` key-values pairs. The following commands take `--omap-key-file`:\r\n\r\n* `getomapval`\r\n* `setomapval`\r\n* `rmomapkey`","short_desc":"provide method to remove binary string keys from rbd_children omap","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1401906,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:47:12Z","package_id":29771,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"","short_desc":"Calamari saltstack dependency to be avoided","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1401936,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:47:18Z","package_id":29771,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"undefined","short_desc":"Calamari to run on all monitor nodes","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1402076,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature, Rebase","last_updated":"2017-04-11T14:13:57Z","package_id":18485,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-04-11T14:14:22Z","release_notes":".ceph rebased to 10.2.5\r\n\r\nThe `ceph` packages have been updated to the upstream version 10.2.5, which provides a number of bug fixes and enhancements over the previous version.","short_desc":"rebase ceph to 10.2.5","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1402080,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Rebase","last_updated":"2017-07-30T15:58:32Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:59:28Z","release_notes":".nfs-ganesha rebased to 2.4.2\r\n\r\nThe `nfs-ganesha` packages have been updated to the upstream version 2.4.2, which provides a number of bug fixes and enhancements over the previous version.","short_desc":"rebase to nfs-ganesha 2.4.2","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text-","id":1404419,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:15:31Z","package_id":39674,"pm_score":0,"priority":"low","qa_whiteboard":"CEPH-10842","reconciled_at":"2017-07-30T15:16:01Z","release_notes":"Fixes a bug which could cause a crash on an OSD restarted after a very long time of being stopped.","short_desc":"map gap causes inconsistent pool.cached_removed_snaps structure","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1408226,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2017-07-30T15:28:06Z","package_id":39675,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:29:42Z","release_notes":"","short_desc":"[librbd]: expose exclusive lock management API","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1410921,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2017-07-30T15:27:01Z","package_id":39675,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:28:12Z","release_notes":"","short_desc":"[rbd-mirror] sporadic image replayer shut down failure","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1414324,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Rebase","last_updated":"2017-07-30T15:52:51Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:53:24Z","release_notes":"","short_desc":"NFS segfaults during file renames in parallel with create","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1414613,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"CodeChange","last_updated":"2017-12-29T09:45:41Z","package_id":39674,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-12-29T09:45:58Z","release_notes":".OSD nodes no longer crash when an I/O error occurs\r\n\r\nPreviously, if an I/O error occurred on one of the objects in an erasure-coded pool during recovery, the primary OSD node of the placement group containing the object hit the runtime check. Consequently, this OSD terminated unexpectedly. With this update, Ceph leaves the object unrecovered without hitting the runtime check. As a result, OSDs no longer crash in such a case.","short_desc":"jewel: osd/ECBackend.cc: 201: FAILED assert(res.errors.empty())","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1415260,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:00:41Z","package_id":39673,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2017-07-30T15:01:54Z","release_notes":".\"Operation not permitted\" errors are no longer incorrectly returned\r\n\r\nWhen using a client whose MDS capabilities are limited by the `path=` parameter, operations in newly created directories in certain cases failed with the \"Operation not permitted\" errors (EPERM). The underlying source code has been modified, and such errors are no longer returned.","short_desc":"CephFS - Unable to create directories recursively when mounted using Ceph-Fuse","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1415640,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:48:19Z","package_id":18485,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"","short_desc":"[ubuntu]: ceph-base package missing from the 'Tools' repo on the latest ubuntu build","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1415981,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T16:04:06Z","package_id":39676,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T16:05:25Z","release_notes":"","short_desc":"NFS segfault seen while creating multiple directory levels with different file size","verified":"FailedQA","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1416160,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"FutureFeature","last_updated":"2017-07-30T16:00:06Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T16:00:56Z","release_notes":".The Ceph Object Gateway now supports the S3 multipart copy operation\r\n\r\nThe Ceph Object Gateway now supports the S3 API for multipart copy, including use of the `x-amz-copy-source` header.\r\n\r\nThe multipart copy operation provides an optimized mechanism for copying existing objects larger than the 5G upload limit of the Amazon Simple Storage Service (S3). For details, see the https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html-single/developer_guide/#copy-multipart-upload[Copy Multipart Upload] section in the Developer Guide for Red Hat Ceph Storage 2.","short_desc":"RFE: Backport support for multipart copy","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1417178,"is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"Reopened","last_updated":"2017-07-30T15:46:36Z","package_id":39676,"pm_score":0,"priority":"urgent","qa_whiteboard":"","reconciled_at":"2017-07-30T15:47:22Z","release_notes":".The Ceph Object Gateway now correctly logs when HTTP clients get disconnected\r\n\r\nDue to incorrect error translation, unexpected disconnections by HTTP clients were incorrectly logged as `HTTP 403: authorization failed` errors. As a consequence, administrators could believe that an actual authentication failure had occurred, and that this failure was visible to clients. With this update, the Ceph Object Gateway handles the error translation correctly and logs a proper error message when the HTTP clients get disconnected.","short_desc":"rgw: fix interface compliance of RGWCivetWeb::write_data()","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1418201,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:55:27Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:56:21Z","release_notes":"","short_desc":"nfs-ganesha crashed when same file is copied from 2 different clients to the same mountpoint","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+,requires_doc_text+","id":1418809,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"Regression","last_updated":"2017-07-30T15:52:33Z","package_id":39676,"pm_score":0,"priority":"medium","qa_whiteboard":"","reconciled_at":"2017-07-30T15:53:25Z","release_notes":".The radosgw-admin utility now supports new options \r\n\r\nThe `radosgw-admin` utility now supports the new `--bypass-gc` and `--inconsistent-index` options. Use these options when deleting indexed buckets to bypass the garbage collector and to ignore bucket index consistency, which improves the speed of the deletion.","short_desc":"Backport RHCS 2.2 : Have a flavor of bucket deletion in radosgw-admin to bypass garbage collection","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1420231,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T16:03:37Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T16:03:59Z","release_notes":"","short_desc":"nfs daemon crashed while deleting all the directories on the mountpoint","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1420530,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:35:12Z","package_id":39675,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:35:45Z","release_notes":"","short_desc":"[RHCeph 2.2/10.2.5-22.el7cp] test_rbd.TestExclusiveLock.test_break_lock ... ./common/WorkQueue.h: In function 'ThreadPool::PointerWQ<T>::~PointerWQ() [with T = librbd::AioImageRequest<librbd::ImageCtx>]'","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1420675,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:49:10Z","package_id":29771,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"","short_desc":"Ceph Installer REST Apis unable to get the cluster details, on an upgraded Ceph setup","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1420793,"is_blocker":0,"is_exception":0,"is_private":1,"is_security":0,"issuetrackers":"","keywords":"Regression","last_updated":"2017-03-14T15:49:15Z","package_id":29771,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"","short_desc":"Ceph pool creation doesn't work with new calamari and ceph","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1421099,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T16:02:24Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T16:03:59Z","release_notes":"","short_desc":"Disabling swift object versioning with \"X-Versions-Location: \" does not disable versioning","verified":"FailedQA","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1422059,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T16:04:28Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T16:05:25Z","release_notes":"","short_desc":"nfs: The corresponding buckets for created directories still remains even after removing them from nfs mountpoint","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1422379,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-03-14T15:49:30Z","package_id":18485,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-03-14T15:55:02Z","release_notes":"","short_desc":"[ubuntu]: python-attrs package required for new calamari-server in MON repo","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1422875,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T16:02:27Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T16:03:59Z","release_notes":"","short_desc":"[NFS:RGW]: While deleting Files in NFS directory, command passes without any error but files still available.","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1423417,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:46:44Z","package_id":39676,"pm_score":0,"priority":"high","qa_whiteboard":"","reconciled_at":"2017-07-30T15:47:22Z","release_notes":"","short_desc":"RGW daemons can not dump core because PR_SET_DUMPABLE is set to 0 after setuid call","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1424687,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:29:47Z","package_id":39675,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:31:12Z","release_notes":"","short_desc":"[rbd-mirror] : after split-brain is detected, unable to resync image using 'rbd mirror image resync <image-spec>'","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1424725,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:53:37Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:54:53Z","release_notes":"","short_desc":"[RGW:Multisite]:- Bucket and objects are not synced to secondary when Master cluster reboots and while I/Os are in progress in master","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1424881,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:26:29Z","package_id":39675,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:26:37Z","release_notes":"","short_desc":"[rbd-mirror]: Image syncing fails on 2nd secondary if 1st secondary has completed syncing before it(rbd-mirror daemon on Primary was stop/started in between)","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1425710,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T16:02:34Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T16:03:59Z","release_notes":"","short_desc":"RGW service fails to start with SSL configured on Ubuntu","verified":"","was_marked_on_qa":1}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1425763,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:44:33Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:45:46Z","release_notes":"","short_desc":"[RGW:NFS]: \"cd\" command to non-existing directory is working without any error message.","verified":"","was_marked_on_qa":0}},{"bug":{"alias":"","bug_severity":"","bug_status":"CLOSED","flags":"ceph-2.y+,devel_ack+,pm_ack+,qa_ack+","id":1425771,"is_blocker":0,"is_exception":0,"is_private":0,"is_security":0,"issuetrackers":"","keywords":"","last_updated":"2017-07-30T15:53:27Z","package_id":39676,"pm_score":0,"priority":"unspecified","qa_whiteboard":"","reconciled_at":"2017-07-30T15:54:53Z","release_notes":"","short_desc":"[RGW:NFS]: NFS-Ganesha service stops while trying to create a directory with '_'","verified":"","was_marked_on_qa":0}}],"to_fetch":[]},"jira_issues":{"errata":{"rhba":{"actual_ship_date":"2017-03-14T15:41:16Z","assigned_to_id":3002003,"batch_id":null,"closed":0,"content_types":["rpm"],"contract":null,"created_at":"2017-01-10T23:16:02Z","current_state_index_id":158326,"current_tps_run":null,"deleted":0,"devel_responsibility_id":3,"doc_complete":1,"docs_responsibility_id":1,"embargo_undated":false,"filelist_changed":0,"filelist_locked":0,"fulladvisory":"RHBA-2017:0514-04","group_id":655,"id":26175,"is_batch_blocker":false,"is_brew":1,"is_valid":1,"issue_date":"2017-03-14T15:30:26Z","mailed":0,"manager_id":3001931,"old_advisory":"RHBA-2017:26175-04","old_delete_product":null,"package_owner_id":3001865,"priority":"normal","product_id":104,"publish_date_override":null,"published":1,"published_shadow":0,"pushcount":3,"pushed":1,"qa_complete":1,"quality_responsibility_id":2,"rating":0,"release_date":null,"reporter_id":3001865,"request":0,"request_rcm_push_comment_id":null,"resolution":"","respin_count":2,"revision":4,"rhn_complete":0,"rhnqa":1,"rhnqa_shadow":0,"security_approved":null,"security_impact":"None","security_sla":null,"severity":"normal","sign_requested":0,"state_machine_rule_set_id":null,"status":"SHIPPED_LIVE","status_updated_at":"2017-03-21T03:24:26Z","supports_multiple_product_destinations":true,"synopsis":"Red Hat Ceph Storage 2.2 bug fix and enhancement update","text_only":false,"text_ready":0,"update_date":"2017-03-17T22:07:06Z","updated_at":"2017-05-22T23:14:00Z","errata_id":514}},"id_field":"key","id_prefix":"jira:","type":"jira_issues","idsfixed":[],"jira_issues":[],"to_fetch":[]}}